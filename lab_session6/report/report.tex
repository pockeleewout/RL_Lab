\documentclass[a4paper]{article}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{minted}
\usepackage{tikz}
\usetikzlibrary{calc,arrows.meta,positioning, automata, shapes}
\usepackage[]{algorithm2e}
\usepackage{paralist}

\hypersetup{
    colorlinks = true,
    citecolor = black,
    urlcolor = blue,
    linkcolor = black,
}
\usepackage[
    backend=biber,
    style=alphabetic,
    citestyle=authoryear
]{biblatex}
\usepackage{csquotes}
\addbibresource{ref.bib}

\newcommand{\question}[2]{
\paragraph{Question #1} -- \textit{#2}

}
\newcommand{\programming}[1]{
\paragraph{Programming} -- \textit{#1}

}

\title{Reinforcement Learning Lab Session 6}
\author{Ewout Pockel√© \\ \href{mailto:ewout.pockele@student.uantwerpen.be}{ewout.pockele@student.uantwerpen.be}}
 
\begin{document}
	\maketitle
	\section*{General Information}
		This lab session will cover all the Reinforcement Learning material from courses 1 to 4. 
		It will be graded lab sessions. 
		You are free to use any content you want regarding the questions, but please refrain from using outside code beyond the previous lab sessions.

		It should be doable in 4 hours, but the deadline will be set to Sunday 29th. 
		As usual, for there on, each day of delay will remove 2.5/10 points from your grade.

		\paragraph{Submission} --
			You will have to submit both a report and code through Blackboard. 
			Use the code available on the git at \url{http://github.com/Louis-Bagot/RL_Lab/lab_session6} or \url{https://github.com/Rajawat23/RL_Lab/lab_session6}.

			Make a copy of this LaTeX document from folder \textbf{report}, and fill it according to the instructions below. 
			Please to not alter this base document, only add your content.

		\question{}{Questions will look like this. For questions, write the answer just below where the (your answer here) appears.}

		\programming{Programming exercises will look like this. 
				For programming exercises, read the \texttt{instructions.md} of the corresponding section, and then fill in the corresponding TODO flags in the code. 
				If this text asks for a plot, copy the plot output of your code in the document, in the figure space provided (in addition to leaving it in the plots folder in the code). 
				If the text asks for an explanation of the results, write it as you would answer a question.}

	\tableofcontents
	\newpage

	\section{Bandits}
		\question{1}{Explain, in a few lines, the k-armed Bandit problem. 
				Give an example of a real-world application, detailing actions and rewards.}
			The k-Armed Bandit problem involves several actions that can be taken, and receiving a (randomized) reward based on the actions taken.

			The actual problem statement is the design of an agent that can maximize the reward received from the actions taken.

			A practical application of this problem would be an investment firm, which would like to invest intelligently to maximize the profit (reward).

		\question{2}{Derive the incremental updates of the Sample Average method.}
			Assume $ A_{n-1} $ exists, and $ \forall k \in \{1,\dots,n\}: \exists a_k $.

			$ A_n = \frac{1}{n} \sum_{k=0}^{n}{a_k} = \frac{a_n}{n} + \frac{1}{n} * \frac{n-1}{n-1} + \sum_{k=0}^{n-1}{a_k} = \frac{a_n}{n} + \frac{n-1}{n} * A_{n-1} = \frac{a_n + (n-1) * A_{n-1}}{n} = A_{n-1} + \frac{a_n - A_{n-1}}{n} $

		\question{3}{Explain, with your own words, the difference between Sample Average and Weighted Average methods}
			The sample average method weigths whole history of rewards equally, while the weigthed average places more emphasis on recent rewards.
			This is also one of the reasons that Sample Average is a good method for static bandits, but performs poorly on moving reward bandits.

			Invers to Sample Averages, Weighted Averages are more suited for moving rewards, since they emphasize recent rewards higher.
			As a side effect of this, these agents perform poorer on static bandits as a result.

		\question{4}{Explain the impact of the hyper-parameters of the following algorithms: $\epsilon$ in $\epsilon$-greedy, $c$ in UCB, and $\alpha$ in Gradient Bandit. 
				Use extreme values as examples.}
			\begin{description}
				\item [$\boldsymbol{\epsilon}$] 
					This parameter influences how the $\epsilon$-greedy agent decides to take action, by specifying the chance with which it takes a random action to learn.

					Where we to specify this parameter as a very small value, say $\epsilon = 10^{-42}$, then the agent will likely (almost) never learn any other action then the first action taken.

					If this value where to be extremely high, say $\epsilon = 1$, then the agent becomes a random agent.
				\item [$\boldsymbol{c}$] 
					This parameter is the initial confidence in each arm, which degrades over time.  
					Changes correlate positively with the length of time it's value will stay significant.  
					This in turn correlates to how long the agent will seemingly pick uniformly between the arms.

					If we where to choose this parameters very small, say we take $c = 2^{-32}$, then the agent will initially choose a random arm and will likely (almost) never choose any other arm, since the confidence starts at 0.

					When we take $c$ to be very large, for example $c = 69^{42}$, then the agent will, for a very long time, pick almost randomly.
				\item [$\boldsymbol{\alpha}$]
					This parameter influences the rate of change for the preferences.

					If this value where to be really large, say $\alpha = 10^{69}$, then the agent might get stuck on the first (few) bandits, since a single reward above baseline will increase their reward immensely.

					On the other hand, a really low reward like $\alpha = 32^{-42}$ will cause the agent to be very slow at learning from the different rewards.
			\end{description}

		\question{5}{Show that the Sample Average method erases the bias introduced by the initialization of the $Q$ estimates. 
				What does this mean for Optimistic Greedy? Show that, despite this, Optimistic Greedy does not work on non-stationary problems.}
			(your answer here)

		\programming{Implement a Sample Average and Weighted Average version of $\epsilon$ greedy on a Non-Stationary k-armed Bandit problem. 
				In order to see results, run the experiment for $10k$ steps, and paste here the resulting performance plot in the Figure \ref{fig:sa_vs_wa} below. 
				Explain your results below.}
			When running the simulation for 10000 runs, we see that the Sample Average agent performs slightly better than the Weighted Average agent, which is unexpected.
			Since this is a stochastic simulation, and I only ran 10000 runs for each agent, it could be that this result is not representative for the general case.

			\begin{figure}[H]
				\centering
				% Change the plot name here:
				\includegraphics[width=8cm]{plots/agent_comparison_perf}
				\caption{Comparison: $\epsilon$-greedy algorithm with Sample Average versus Weighted Average updates.}
				\label{fig:sa_vs_wa}
			\end{figure}{}


	\section{Markov Decision Processes}
		For questions where a drawing (MDP or diagram) is required, you can use whichever of the following methods:
		\begin{itemize}
			\item a (properly cropped and clear) photo of a drawing on paper. 
				Make sure everything is readable.
			\item a tikz diagram, i.e. the plotting tool for LaTeX (if you know how it works. Don't learn for this report otherwise, tikz takes an eternity)
			\item \href{www.mathcha.io}{Mathcha}, which can generate tikz or pngs. (recommended)
		\end{itemize}{}

		\question{1}{Define a Markov Decision Process, and the goal of the Reinforcement Learning problem.}
			A Markov Decision Process is a Markov process with external inputs to drive state transitions that have a stochastic outcome.
			The goal of using these processes is to maximize the long-term reward of an AI agent.

		\question{2}{Show that the MDP framework generalizes over Bandits by drawing the Bandits problem as a MDP with reward distributions $r_a$ for each action $a$. 
				Paste your drawing on Figure \ref{fig:bandit_mdp}. 
				Shortly explain your submission.}

			\begin{figure}[H]
				\centering
				% Change the plot name here:
				\includegraphics[width=8cm]{plots/IMG_0257}
				\caption{The MDP corresponding to the Bandit problem with reward distributions $r_a$, $\forall$ actions $a$}.
				\label{fig:bandit_mdp}
			\end{figure}{}
			When the agent chooses a bandit $a_i$, it will receive a reward from the distribution $r_{a_i}$.
			For a multi-armed bandit this can be modelled by defining multiple actions with different reward distributions.
			If you want to do non-stationary distributions, you have to discretize the stochastic distributions used for the variation.
			Then one introduces a state for every possible combination of reward distributions for the different bandits in the multi-armed bandit.


		\question{3}{Turn the following statement into a MDP, with states and transitions with actions (named), probabilities and rewards. 
				Paste the graph on Figure \ref{fig:q3}; pick real values for both rewards and probabilities (no unknowns). 
				Shortly explain your submission after the statement and plot.}

			\textbf{Statement:}  \emph{You go to the university using Velo -- Antwerp's shared bikes. 
				There are three stations where you can drop off your bike on the way: the park, furthest from the university; the cemetery, second furthest; and the university station, right in front of your destination.
				You want to take the least time possible to go to the university, and you take much longer walking than biking.
				At any station, you can either decide to drop off your bike and walk to the university, or continue to the next station.
				However, it sometimes happens that the stations are full - you cannot drop off your bike there. You can either go back, or, if possible, continue.
				You notice that the amount of free spots in the first stations often aligns with the amount of free spots in the following stations - or is less. 
				In order to decide whether you should drop off your bike or not, you take note of the last station's number of free spots - it can either be a lot, or a few, or none.
				When you have to go back, we assume that people could've come to pick or drop bikes, so the transition doesn't depend on the previous state of the station.}

			\begin{figure}[H]
				\centering
				% Change the plot name here:
				\includegraphics[width=8cm]{plots/mdp_3}
				\caption{The MDP corresponding to the statement of Question 3}.
				\label{fig:q3}
			\end{figure}{}

			We have 9 main states, and a ``Parked'' end-state. We have 3 states for each location; the $P$ park-states, the $C$ cemetery-states and the $U$ university-states.
			Each have a low ($P_l, C_l, U_l$), medium ($P_m, C_m, U_m$) and high ($P_h, C_h, U_h$) occupancy rate.
			We have 2 possible actions: continue ($c$) or park($p$).

			When we continue to the next stop, there is a chance the stop is completely full, and we automatically return to the previous state.
			There is no sense in continuing to the next stop, since the occupancy rate is either the same or higher, which automatically means it is also full.
			When we move, there is a negative reward for losing time.

			When we decide to park, there is always a space available, and thus we have a $100\%$ chance of parking and gaining the reward.

		\question{4}{RL has been widely popular lately because of its combination with Deep Learning (using Neural Nets as policy or value function approximators), leading to incredible performances on difficult environments like video games. 
				One such game is the first Mario World. 
				Show how the MDP can look like for the frames of the game. 
				Where can stochasticity be involved?}
			Since encoding a game state into an MDP would take too much space to visualize, I decided to keep this a mental exercise.

			To encode the frames of the Super Mario video game, one could start by taking a number ``beams'', originating from the player character and having evenly spaced angles between them.
			One could take the distances along these beams from the player character to the nearest object, and encode them into the states of the MDP.
			Also encoded into the state could be the types of objects colliding with the beams.

			Some game-specific optimizations could be to take into account hardware limitations, from which we know how many enemies can be on-screen at once.
			You could then encode the positions and types of all the enemies on the screen into the state.

			The stochasticity comes into play when we transition between states, since an enemy could take a multitide of actions.


	\section{Control}
		In lab session 2 and 3, the Value Iteration algorithm was stopped after a maximum number of iterations. 
		However, this is not the natural stopping condition of the algorithm: it should stop when the value estimates have converged: $V_k = v*$. 
		When implementing this, we define convergence of the $V_{k-1},V_k,V_{k+1}..$ stream of vector values as $$ \vert\vert V_{k+1} - V_k \vert\vert_2 < \delta $$
		Where $\delta$ is an arbitrary small constant ($\delta = 0.01$ in the code). 
		The number of iterations of Value Iteration to convergence is a measure of the algorithm's performance.

		Policy Iteration alternates evaluating a policy $\pi$ until convergence to $V_\pi$, and updating the policy to be greedy over the new values, $\pi ^\prime = greedy(v_\pi)$. 
		We define \textit{convergence in policy} as  $\pi^\prime = \pi$ (same action in all states), and \textit{convergence in value} as $$ \vert\vert V_{\pi ^\prime} - V_\pi \vert\vert_2 < \delta $$

		Value Iteration only converges in value, as there is no explicit policy. 
		When comparing convergence speed in value of Value Iteration vs Policy Iteration, make sure to compare the number of single sweeps over the state space! (iterations)

		\programming{Implement Value Iteration on the course's diamond/pit Gridworld environment (course and Lab session 2). 
				You can reuse Environment code from before.}

		\programming{Implement Policy Iteration on the course's diamond/pit Gridworld environment.}

		\question{1}{Discuss and compare the performances of both algorithms. 
				Under what circumstances can one come on top of the other?}
			When comparing Value Iteration and Policy iteration, one has to keep in mind that they both compute the same thing, but do so in slightly different ways.

			Value Iteration is, in the general case, slower than Policy iteration.
			This stems from the fact that during value iteration we evaluate all the actions in every state, thus we have a complexity of $O(|S| * |A|)$ for each iteration on $V$.
			On the other hand, Policy Iteration has a complexity of $O(|S|)$ for each iteration on $V_{\pi}$, and a complexity of $O(|A|)$ for each iteration on $\pi$.

			When the system the algorithm is being used on has a lot of actions, then Policy iteration has a clear advantage.
			In the situation an MDP has a lot of states and very few actions, I theorise there could be an advantage for Value Iteration, since it will evaluate all actions at once, and not in turn like Policy Iteration.

		\question{3}{Explain the fundamental differences between QLearning and the Value Iteration / Policy Iteration algorithms. 
				Can you see why QLearning is more interesting in the general case?}
			The fundamental difference between Value-/Policy- Iteration and Q-Learning is that Value- and Policy- Iteration are offline algorithms, and Q-Learning is an online algorithm.
			Another fundamental difference is that Q-Learning does not try to learn the value of each state, but the q-values of each action directly.

	\section{Bonus}
		\programming{\textbf{BONUS, 1.5pts} Implement the Gridworld drawn on Figure \ref{fig:river_crossing}: a river crossing. 
				The actions are up, down, left, right and "do nothing". 
				The agent needs to cross a river from the lower left corner (state S) to the upper right corner (state G). 
				This $3\times 5$ Gridworld is divided on the 2nd row by a river that might push the agent right upon entering by one, two or three squares, with corresponding probabilities $0.2, 0.5$ and $0.3$. 
				If the agent is pushed off to the far right in the river, the episode ends with reward $-1$. 
				If the agent reaches the goal, the reward is $+1$. 
				Note that it is the transition to the state (i.e. "right" from (0,3) to G=(0,4)) that yields the reward, and states G and red (1,4) are terminal.}

		\begin{figure}[H]
			\centering
			% Change the plot name here:
			\includegraphics[width=6cm]{plots/river.png}
			\caption{The River Crossing MDP}.
			\label{fig:river_crossing}
		\end{figure}{}


	\printbibliography
\end{document}

