\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{A2C\_PPO}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{lab-session-8}{%
\section{Lab session 8}\label{lab-session-8}}

\hypertarget{what-is-a2c}{%
\subsubsection{What is A2C?}\label{what-is-a2c}}

Since the beginning of this course, we've studied two different
reinforcement learning methods:

\begin{itemize}
\tightlist
\item
  \textbf{Value based methods} (Q-learning, Deep Q-learning): where we
  learn a value function that will map each state action pair to a
  value.
\item
  \textbf{Policy based methods} (REINFORCE with Policy Gradients): where
  we directly optimize the policy without using a value function.
\end{itemize}

But both of these methods have big drawbacks. That's why, today, we'll
study a new type of Reinforcement Learning method which we can call a
``hybrid method'': Actor Critic. We'll using two neural networks:

\begin{itemize}
\tightlist
\item
  \textbf{Actor}: Controls how our agent behaves (policy-based)
\item
  \textbf{Critic}: Measures how good the state is (value-based)
\end{itemize}

The Actor Critic model is a better score function. Instead of waiting
until the end of the episode as we do in Monte Carlo REINFORCE, we make
an update at each step (TD Learning).

At the beginning, you don't know how to play, so you try some action
randomly. The Critic observes your action and provides feedback.
Learning from this feedback, you'll update your policy and be better at
playing that game.

On the other hand, your friend (Critic) will also update their own way
to provide feedback so it can be better next time. As we can see, the
idea of Actor Critic is to have two function approximator, the policy
(actor) and the value function (critic). We estimate both with neural
networks.

Because we have two models (Actor and Critic) that must be trained, it
means that we have two set of weights that must be optimized separately.

\hypertarget{submitting-the-code-and-experiment-runs}{%
\subsubsection{Submitting the code and experiment
runs}\label{submitting-the-code-and-experiment-runs}}

In order to turn in your code and report, create 3 folders that contains
the following: * Cartpole * A2C and PPO code using cartpole. * Plots *
LunarLander * A2C and PPO code using LunarLander-v2 env * Plots * Report
* Report

A scientific report explaining * Difference between A2C and PPO? * Try
solving LunarLander-v2 env. using already implemented code for cartpole
Set N\_TRIALS = 1 and REWARD\_THRESHOLD = 100. You are only allowed to
change MAX\_EPISODES. * Compare the performance of A2C and PPO in
different envs * If any difference please explain why? * What can be
potential improvements?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{distributions} \PY{k}{as} \PY{n+nn}{distributions}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{gym}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{clear\PYZus{}output}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{declare-env}{%
\paragraph{Declare env}\label{declare-env}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{env\PYZus{}id} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LunarLander\PYZhy{}v2}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{train\PYZus{}env} \PY{o}{=} \PY{n}{gym}\PY{o}{.}\PY{n}{make}\PY{p}{(}\PY{n}{env\PYZus{}id}\PY{p}{)}
\PY{n}{eval\PYZus{}env} \PY{o}{=} \PY{n}{gym}\PY{o}{.}\PY{n}{make}\PY{p}{(}\PY{n}{env\PYZus{}id}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{SEED} \PY{o}{=} \PY{l+m+mi}{1236}
\PY{n}{train\PYZus{}env}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{SEED}\PY{p}{)}\PY{p}{;}
\PY{n}{eval\PYZus{}env}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n+nb}{int}\PY{p}{(}\PY{n}{SEED}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{SEED}\PY{p}{)}\PY{p}{;}
\PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{n}{SEED}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{BaseModel}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{output\PYZus{}dim}\PY{p}{,} \PY{n}{dropout} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        This is a basic Artificial Neural Network that can be used for both the Actor and the Critic.}
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        input\PYZus{}dim: int}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        hidden\PYZus{}dim: int}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        output\PYZus{}dim: int}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        dropout: float}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc\PYZus{}1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}dim}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc\PYZus{}2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}dim}\PY{p}{,} \PY{n}{output\PYZus{}dim}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{dropout}\PY{p}{)}
        
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Prediction of the Neural Network given an input x (in RL, x is a state).}
\PY{l+s+sd}{        The Network uses a dropout layer (to help generalize), and the ReLU activation function.}
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        x: tensor}
\PY{l+s+sd}{            input, i.e. state}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        x: tensor}
\PY{l+s+sd}{            the network\PYZsq{}s prediction (output) for this input.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} TODO: write forward pass for NN}
        \PY{c+c1}{\PYZsh{} Hint: it should have layers, dropout, relu.}
        \PY{n}{out} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc\PYZus{}1}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{out} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{out}\PY{p}{)}
        \PY{n}{out} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n}{out}\PY{p}{)}
        \PY{n}{out} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc\PYZus{}2}\PY{p}{(}\PY{n}{out}\PY{p}{)}
\PY{c+c1}{\PYZsh{}         print(\PYZdq{}BaseModel.forward(), out:\PYZdq{}, out)}

        \PY{k}{return} \PY{n}{out}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{ActorCritic}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{actor}\PY{p}{,} \PY{n}{critic}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        This is a joint model, with two ANNs within.}
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        actor: BaseModel instance}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        critic: BaseModel instance}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{actor} \PY{o}{=} \PY{n}{actor}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{critic} \PY{o}{=} \PY{n}{critic}
        
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} }
\PY{l+s+sd}{        The output of the ActorCritic model is the concatenation of the actor and critic\PYZsq{}s outputs.}
\PY{l+s+sd}{        Since the actor is a policy, we convert the output into probabilities using a softmax function.}
\PY{l+s+sd}{        Parameters}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        state: tensor}
\PY{l+s+sd}{            model input, i.e. state the agent is in}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        Returns}
\PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        action\PYZus{}pred, value\PYZus{}pred: tensor, tensor}
\PY{l+s+sd}{            the network\PYZsq{}s prediction (output) for this input.}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}

        \PY{n}{action\PYZus{}pred} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{actor}\PY{p}{(}\PY{n}{state}\PY{p}{)}
        \PY{n}{value\PYZus{}pred} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{critic}\PY{p}{(}\PY{n}{state}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} TODO: convert actions to probabilities using softmax}
        \PY{n}{action\PYZus{}pred} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{action\PYZus{}pred}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{k}{return} \PY{n}{action\PYZus{}pred}\PY{p}{,} \PY{n}{value\PYZus{}pred}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{note-reinitialise-agent-again-if-you-change-any-hyperparameter.-dont-retrain-same-agent.}{%
\subsubsection{Note: Reinitialise agent again if you change any
hyperparameter. Don't retrain same
agent.}\label{note-reinitialise-agent-again-if-you-change-any-hyperparameter.-dont-retrain-same-agent.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{INPUT\PYZus{}DIM} \PY{o}{=} \PY{n}{train\PYZus{}env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{HIDDEN\PYZus{}DIM} \PY{o}{=} \PY{l+m+mi}{128}

\PY{c+c1}{\PYZsh{} TODO: What should be the output dimention for actor and critic.}
\PY{c+c1}{\PYZsh{} HINT: actor controls policy and critic outputs only values}
\PY{n}{OUTPUT\PYZus{}DIM\PYZus{}ACTOR} \PY{o}{=} \PY{n}{train\PYZus{}env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{n}
\PY{n}{OUTPUT\PYZus{}DIM\PYZus{}CRITIC} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{actor} \PY{o}{=} \PY{n}{BaseModel}\PY{p}{(}\PY{n}{INPUT\PYZus{}DIM}\PY{p}{,} \PY{n}{HIDDEN\PYZus{}DIM}\PY{p}{,} \PY{n}{OUTPUT\PYZus{}DIM\PYZus{}ACTOR}\PY{p}{)}
\PY{n}{critic} \PY{o}{=} \PY{n}{BaseModel}\PY{p}{(}\PY{n}{INPUT\PYZus{}DIM}\PY{p}{,} \PY{n}{HIDDEN\PYZus{}DIM}\PY{p}{,} \PY{n}{OUTPUT\PYZus{}DIM\PYZus{}CRITIC}\PY{p}{)}

\PY{n}{agent} \PY{o}{=} \PY{n}{ActorCritic}\PY{p}{(}\PY{n}{actor}\PY{p}{,} \PY{n}{critic}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{init\PYZus{}weights}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Initializes the ANNs weights with a relevant distribution. \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{m}\PY{p}{)} \PY{o}{==} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{:}
        \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{xavier\PYZus{}normal\PYZus{}}\PY{p}{(}\PY{n}{m}\PY{o}{.}\PY{n}{weight}\PY{p}{)}
        \PY{n}{m}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{agent}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{init\PYZus{}weights}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
ActorCritic(
  (actor): BaseModel(
    (fc\_1): Linear(in\_features=8, out\_features=128, bias=True)
    (fc\_2): Linear(in\_features=128, out\_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (critic): BaseModel(
    (fc\_1): Linear(in\_features=8, out\_features=128, bias=True)
    (fc\_2): Linear(in\_features=128, out\_features=1, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{LEARNING\PYZus{}RATE} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{agent}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{n}{LEARNING\PYZus{}RATE}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{discount\PYZus{}factor}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Performs a single training step over an episode.}

\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    agent: ActorCritic}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    optimizer: PyTorch optimizer}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    discount\PYZus{}factor: float}
\PY{l+s+sd}{        discount gamma}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    policy\PYZus{}loss: float }
\PY{l+s+sd}{        loss of the policy (actor)}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    value\PYZus{}loss: float}
\PY{l+s+sd}{        loss of the value function approximator (critic)}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    episode\PYZus{}reward: float}
\PY{l+s+sd}{        reward for this episode}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{agent}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{log\PYZus{}prob\PYZus{}actions} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{values} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{done} \PY{o}{=} \PY{k+kc}{False}
    \PY{n}{episode\PYZus{}reward} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}

    \PY{k}{while} \PY{o+ow}{not} \PY{n}{done}\PY{p}{:}

        \PY{n}{state} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{FloatTensor}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} TODO: get action}
        \PY{n}{action\PYZus{}prob}\PY{p}{,} \PY{n}{value\PYZus{}pred} \PY{o}{=} \PY{n}{agent}\PY{p}{(}\PY{n}{state}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} TODO: get value}
\PY{c+c1}{\PYZsh{}         value\PYZus{}pred = }
         
        \PY{n}{dist} \PY{o}{=} \PY{n}{distributions}\PY{o}{.}\PY{n}{Categorical}\PY{p}{(}\PY{n}{action\PYZus{}prob}\PY{p}{)}

        \PY{n}{action} \PY{o}{=} \PY{n}{dist}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{log\PYZus{}prob\PYZus{}action} \PY{o}{=} \PY{n}{dist}\PY{o}{.}\PY{n}{log\PYZus{}prob}\PY{p}{(}\PY{n}{action}\PY{p}{)}
        
        \PY{k}{if} \PY{n}{env\PYZus{}id} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Pendulum\PYZhy{}v0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
            \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}

        \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{log\PYZus{}prob\PYZus{}action}\PY{p}{)}
        \PY{n}{values}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{value\PYZus{}pred}\PY{p}{)}
        \PY{n}{rewards}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{reward}\PY{p}{)}

        \PY{n}{episode\PYZus{}reward} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
    
    \PY{n}{log\PYZus{}prob\PYZus{}actions} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{p}{)}
    \PY{n}{values} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{values}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{n}{returns} \PY{o}{=} \PY{n}{calculate\PYZus{}returns}\PY{p}{(}\PY{n}{rewards}\PY{p}{,} \PY{n}{discount\PYZus{}factor}\PY{p}{)}
    \PY{n}{advantages} \PY{o}{=} \PY{n}{calculate\PYZus{}advantages}\PY{p}{(}\PY{n}{returns}\PY{p}{,} \PY{n}{values}\PY{p}{)}
    
    \PY{n}{policy\PYZus{}loss}\PY{p}{,} \PY{n}{value\PYZus{}loss} \PY{o}{=} \PY{n}{update\PYZus{}policy}\PY{p}{(}\PY{n}{advantages}\PY{p}{,} \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{p}{,} \PY{n}{returns}\PY{p}{,} \PY{n}{values}\PY{p}{)}

    \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{policy\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
    \PY{n}{value\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

    \PY{k}{return} \PY{n}{policy\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{value\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{episode\PYZus{}reward}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{calculate\PYZus{}returns}\PY{p}{(}\PY{n}{rewards}\PY{p}{,} \PY{n}{discount\PYZus{}factor}\PY{p}{,} \PY{n}{normalize} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Function to calculate rewards in time step order and normalize them.}
\PY{l+s+sd}{    Normalizing stabilizes the results.}
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    rewards: list of floats}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    discount\PYZus{}factor: float}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    returns: tensor}
\PY{l+s+sd}{        tensor of returns G\PYZus{}t in time\PYZhy{}step order}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} TODO: calculate future rewards}
    \PY{n}{rewards}\PY{o}{.}\PY{n}{reverse}\PY{p}{(}\PY{p}{)}
    \PY{n}{returns} \PY{o}{=} \PY{p}{[}\PY{n}{rewards}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}
    \PY{k}{while} \PY{n+nb}{len}\PY{p}{(}\PY{n}{rewards}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{n}{returns}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{rewards}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{+} \PY{n}{discount\PYZus{}factor} \PY{o}{*} \PY{n}{returns}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

    \PY{n}{returns}\PY{o}{.}\PY{n}{reverse}\PY{p}{(}\PY{p}{)}

    \PY{n}{returns} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{returns}\PY{p}{)}

    \PY{k}{if} \PY{n}{normalize}\PY{p}{:}
        \PY{n}{returns} \PY{o}{=} \PY{p}{(}\PY{n}{returns} \PY{o}{\PYZhy{}} \PY{n}{returns}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{returns}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}

    \PY{k}{return} \PY{n}{returns}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{calculate\PYZus{}advantages}\PY{p}{(}\PY{n}{returns}\PY{p}{,} \PY{n}{values}\PY{p}{,} \PY{n}{normalize} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Computes the advantage for all actions. }
\PY{l+s+sd}{    Reminder: the Advantage function for an action a is A(s,a) = Q(s,a) \PYZhy{} V(s)}
\PY{l+s+sd}{    Normalizing stabilizes the results.}
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    returns: tensor}
\PY{l+s+sd}{        Returns G\PYZus{}t during an episode}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    values: tensor}
\PY{l+s+sd}{        Value estimates V(s\PYZus{}t)}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    advantages: tensor}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} TODO: calculate advantage}
    \PY{n}{advantages} \PY{o}{=} \PY{n}{returns} \PY{o}{\PYZhy{}} \PY{n}{values}
    
    \PY{c+c1}{\PYZsh{} TODO: write code to normalize the values}
    \PY{k}{if} \PY{n}{normalize}\PY{p}{:}
        \PY{n}{advantages} \PY{o}{=} \PY{p}{(}\PY{n}{advantages} \PY{o}{\PYZhy{}} \PY{n}{advantages}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{advantages}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
  
        
    \PY{k}{return} \PY{n}{advantages}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{update\PYZus{}policy}\PY{p}{(}\PY{n}{advantages}\PY{p}{,} \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{p}{,} \PY{n}{returns}\PY{p}{,} \PY{n}{values}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Function to update your policy based on your actor and critic loss.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    advantages: tensor}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    log\PYZus{}prob\PYZus{}actions: tensor}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    returns: tensor}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    values: tensor}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    optimizer: adam instance}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}

\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{n}{advantages} \PY{o}{=} \PY{n}{advantages}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}
    \PY{n}{returns} \PY{o}{=} \PY{n}{returns}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} TODO: calculate policy loss based on advantages and log\PYZus{}prob\PYZus{}actions.}
    \PY{n}{policy\PYZus{}loss} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{log\PYZus{}prob\PYZus{}actions} \PY{o}{*} \PY{n}{returns}
    \PY{n}{policy\PYZus{}loss} \PY{o}{=} \PY{n}{policy\PYZus{}loss}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} TODO: calculate value loss based on Mean Absolute Error}
    \PY{n}{value\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{l1\PYZus{}loss}\PY{p}{(}\PY{n}{values}\PY{p}{,} \PY{n}{returns}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{policy\PYZus{}loss}\PY{p}{,} \PY{n}{value\PYZus{}loss}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{evaluate}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{vis}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Function to evaluate your agent\PYZsq{}s performance.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{agent}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{done} \PY{o}{=} \PY{k+kc}{False}
    \PY{n}{episode\PYZus{}reward} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
    \PY{k}{if} \PY{n}{vis}\PY{p}{:} \PY{n}{env}\PY{o}{.}\PY{n}{render}\PY{p}{(}\PY{p}{)}
    \PY{k}{while} \PY{o+ow}{not} \PY{n}{done}\PY{p}{:}

        \PY{n}{state} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{FloatTensor}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

        \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        
            \PY{n}{action\PYZus{}prob}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{agent}\PY{p}{(}\PY{n}{state}\PY{p}{)}
                
        \PY{n}{action} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{action\PYZus{}prob}\PY{p}{,} \PY{n}{dim} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                
        \PY{k}{if} \PY{n}{env\PYZus{}id} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Pendulum\PYZhy{}v0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
            \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}

        \PY{n}{episode\PYZus{}reward} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
        
    \PY{k}{return} \PY{n}{episode\PYZus{}reward}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plot}\PY{p}{(}\PY{n}{frame\PYZus{}idx}\PY{p}{,} \PY{n}{train\PYZus{}rewards}\PY{p}{,} \PY{n}{policy\PYZus{}loss}\PY{p}{,} \PY{n}{value\PYZus{}loss}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Plots the running reward and losses.}
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    frame\PYZus{}idx: int}
\PY{l+s+sd}{        frame id}
\PY{l+s+sd}{    rewards: int}
\PY{l+s+sd}{        accumulated reward}
\PY{l+s+sd}{    losses: int}
\PY{l+s+sd}{        loss}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{clear\PYZus{}output}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{131}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{frame }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{. reward: }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{frame\PYZus{}idx}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}rewards}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}rewards}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{132}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{policy loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{policy\PYZus{}loss}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{value\PYZus{}loss}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{MAX\PYZus{}EPISODES} \PY{o}{=} \PY{l+m+mi}{500}
\PY{n}{DISCOUNT\PYZus{}FACTOR} \PY{o}{=} \PY{l+m+mf}{0.99}
\PY{n}{N\PYZus{}TRIALS} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{REWARD\PYZus{}THRESHOLD} \PY{o}{=} \PY{l+m+mi}{100}

\PY{n}{policy\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{value\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{train\PYZus{}rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{test\PYZus{}rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}

\PY{k}{for} \PY{n}{episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{MAX\PYZus{}EPISODES}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    
    \PY{n}{policy\PYZus{}loss}\PY{p}{,} \PY{n}{value\PYZus{}loss}\PY{p}{,} \PY{n}{train\PYZus{}reward} \PY{o}{=} \PY{n}{train}\PY{p}{(}\PY{n}{train\PYZus{}env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{DISCOUNT\PYZus{}FACTOR}\PY{p}{)}
    \PY{n}{test\PYZus{}reward} \PY{o}{=} \PY{n}{evaluate}\PY{p}{(}\PY{n}{eval\PYZus{}env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{k+kc}{False}\PY{p}{)}
    
    \PY{n}{train\PYZus{}rewards}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{train\PYZus{}reward}\PY{p}{)}
    \PY{n}{test\PYZus{}rewards}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{test\PYZus{}reward}\PY{p}{)}
    \PY{n}{policy\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{policy\PYZus{}loss}\PY{p}{)}
    \PY{n}{value\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{value\PYZus{}loss}\PY{p}{)}
    
    \PY{n}{mean\PYZus{}train\PYZus{}rewards} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}rewards}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{N\PYZus{}TRIALS}\PY{p}{:}\PY{p}{]}\PY{p}{)}
    \PY{n}{mean\PYZus{}test\PYZus{}rewards} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}rewards}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{N\PYZus{}TRIALS}\PY{p}{:}\PY{p}{]}\PY{p}{)}
    
    \PY{k}{if} \PY{n}{episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{n}{plot}\PY{p}{(}\PY{n}{episode}\PY{p}{,} \PY{n}{train\PYZus{}rewards}\PY{p}{,} \PY{n}{policy\PYZus{}losses}\PY{p}{,} \PY{n}{value\PYZus{}losses}\PY{p}{)}

    \PY{k}{if} \PY{n}{mean\PYZus{}test\PYZus{}rewards} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{REWARD\PYZus{}THRESHOLD}\PY{p}{:}
        \PY{n}{plot}\PY{p}{(}\PY{n}{episode}\PY{p}{,} \PY{n}{train\PYZus{}rewards}\PY{p}{,} \PY{n}{policy\PYZus{}losses}\PY{p}{,} \PY{n}{value\PYZus{}losses}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Reached reward threshold in }\PY{l+s+si}{\PYZob{}}\PY{n}{episode}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ episodes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{k}{break}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Reached reward threshold in 152 episodes
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plot\PYZus{}evaluate}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}rewards}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Reward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}rewards}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Reward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Reward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{hlines}\PY{p}{(}\PY{n}{REWARD\PYZus{}THRESHOLD}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}rewards}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plot\PYZus{}evaluate}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{ppo}{%
\subsection{PPO}\label{ppo}}

    The RL algorithms have many moving parts that are hard to debug, and
they require substantial effort in tuning in order to get good results.
PPO strikes a balance between ease of implementation, sample complexity,
and ease of tuning, trying to compute an update at each step that
minimizes the cost function while ensuring the deviation from the
previous policy is relatively small. The central idea for Proximal
Policy Optimization is to avoid having too large policy update

PPO uses an adaptive KL penalty to control the change of the policy at
each iteration. It uses an objective function not typically found in
other algorithms.

Here we implement PPO agent in A2C style. Which means it follows same
actor critic implementation.

    \hypertarget{reinitialise-your-agents}{%
\paragraph{Reinitialise your agents}\label{reinitialise-your-agents}}

\hypertarget{todo-reuse-a2c-methods-to-validate-if-ppo-is-running-correctly-with-cartpole-env.}{%
\subparagraph{TODO: Reuse A2C methods to validate if PPO is running
correctly with
cartpole-env.}\label{todo-reuse-a2c-methods-to-validate-if-ppo-is-running-correctly-with-cartpole-env.}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{agent} \PY{o}{=} \PY{n}{ActorCritic}\PY{p}{(}\PY{n}{actor}\PY{p}{,} \PY{n}{critic}\PY{p}{)}
\PY{n}{agent}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{init\PYZus{}weights}\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{agent}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{n}{LEARNING\PYZus{}RATE}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{update\PYZus{}policy\PYZus{}ppo}\PY{p}{(}\PY{n}{agent}\PY{p}{,} \PY{n}{states}\PY{p}{,} \PY{n}{actions}\PY{p}{,} \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{p}{,} \PY{n}{advantages}\PY{p}{,} \PY{n}{returns}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{ppo\PYZus{}steps}\PY{p}{,} \PY{n}{ppo\PYZus{}clip}\PY{p}{)}\PY{p}{:}
    
    \PY{n}{total\PYZus{}policy\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0} 
    \PY{n}{total\PYZus{}value\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    
    \PY{n}{advantages} \PY{o}{=} \PY{n}{advantages}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}
    \PY{n}{log\PYZus{}prob\PYZus{}actions} \PY{o}{=} \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}
    \PY{n}{actions} \PY{o}{=} \PY{n}{actions}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{ppo\PYZus{}steps}\PY{p}{)}\PY{p}{:}
                
        \PY{c+c1}{\PYZsh{}get new log prob of actions for all input states}
        \PY{n}{action\PYZus{}prob}\PY{p}{,} \PY{n}{value\PYZus{}pred} \PY{o}{=} \PY{n}{agent}\PY{p}{(}\PY{n}{states}\PY{p}{)}
        \PY{n}{value\PYZus{}pred} \PY{o}{=} \PY{n}{value\PYZus{}pred}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{dist} \PY{o}{=} \PY{n}{distributions}\PY{o}{.}\PY{n}{Categorical}\PY{p}{(}\PY{n}{action\PYZus{}prob}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}new log prob using old actions}
        \PY{n}{new\PYZus{}log\PYZus{}prob\PYZus{}actions} \PY{o}{=} \PY{n}{dist}\PY{o}{.}\PY{n}{log\PYZus{}prob}\PY{p}{(}\PY{n}{actions}\PY{p}{)}
        
        \PY{n}{policy\PYZus{}ratio} \PY{o}{=} \PY{p}{(}\PY{n}{new\PYZus{}log\PYZus{}prob\PYZus{}actions} \PY{o}{\PYZhy{}} \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{p}{)}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{p}{)}
                
        \PY{n}{policy\PYZus{}loss\PYZus{}1} \PY{o}{=} \PY{n}{policy\PYZus{}ratio} \PY{o}{*} \PY{n}{advantages}
        \PY{n}{policy\PYZus{}loss\PYZus{}2} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{clamp}\PY{p}{(}\PY{n}{policy\PYZus{}ratio}\PY{p}{,} \PY{n+nb}{min} \PY{o}{=} \PY{l+m+mf}{1.0} \PY{o}{\PYZhy{}} \PY{n}{ppo\PYZus{}clip}\PY{p}{,} \PY{n+nb}{max} \PY{o}{=} \PY{l+m+mf}{1.0} \PY{o}{+} \PY{n}{ppo\PYZus{}clip}\PY{p}{)} \PY{o}{*} \PY{n}{advantages}
        
        \PY{n}{policy\PYZus{}loss} \PY{o}{=} \PY{o}{\PYZhy{}} \PY{n}{torch}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{policy\PYZus{}loss\PYZus{}1}\PY{p}{,} \PY{n}{policy\PYZus{}loss\PYZus{}2}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{value\PYZus{}loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{smooth\PYZus{}l1\PYZus{}loss}\PY{p}{(}\PY{n}{returns}\PY{p}{,} \PY{n}{value\PYZus{}pred}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}

        \PY{n}{policy\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{value\PYZus{}loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}

        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
    
        \PY{n}{total\PYZus{}policy\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{policy\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
        \PY{n}{total\PYZus{}value\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{value\PYZus{}loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{total\PYZus{}policy\PYZus{}loss} \PY{o}{/} \PY{n}{ppo\PYZus{}steps}\PY{p}{,} \PY{n}{total\PYZus{}value\PYZus{}loss} \PY{o}{/} \PY{n}{ppo\PYZus{}steps}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train\PYZus{}ppo}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{discount\PYZus{}factor}\PY{p}{,} \PY{n}{ppo\PYZus{}steps}\PY{p}{,} \PY{n}{ppo\PYZus{}clip}\PY{p}{)}\PY{p}{:}
        
    \PY{n}{agent}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
        
    \PY{n}{states} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{actions} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{log\PYZus{}prob\PYZus{}actions} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{values} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{done} \PY{o}{=} \PY{k+kc}{False}
    \PY{n}{episode\PYZus{}reward} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}

    \PY{k}{while} \PY{o+ow}{not} \PY{n}{done}\PY{p}{:}

        \PY{n}{state} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{FloatTensor}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

        \PY{c+c1}{\PYZsh{}append state here, not after we get the next state from env.step()}
        \PY{n}{states}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{state}\PY{p}{)}
        
        \PY{n}{action\PYZus{}prob}\PY{p}{,} \PY{n}{value\PYZus{}pred} \PY{o}{=} \PY{n}{agent}\PY{p}{(}\PY{n}{state}\PY{p}{)}
                
        \PY{n}{dist} \PY{o}{=} \PY{n}{distributions}\PY{o}{.}\PY{n}{Categorical}\PY{p}{(}\PY{n}{action\PYZus{}prob}\PY{p}{)}
        
        \PY{n}{action} \PY{o}{=} \PY{n}{dist}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{log\PYZus{}prob\PYZus{}action} \PY{o}{=} \PY{n}{dist}\PY{o}{.}\PY{n}{log\PYZus{}prob}\PY{p}{(}\PY{n}{action}\PY{p}{)}
        
        \PY{k}{if} \PY{n}{env\PYZus{}id} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Pendulum\PYZhy{}v0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
            \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}

        \PY{n}{actions}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{action}\PY{p}{)}
        \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{log\PYZus{}prob\PYZus{}action}\PY{p}{)}
        \PY{n}{values}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{value\PYZus{}pred}\PY{p}{)}
        \PY{n}{rewards}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{reward}\PY{p}{)}
        
        \PY{n}{episode\PYZus{}reward} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
    
    \PY{n}{states} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{states}\PY{p}{)}
    \PY{n}{actions} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{actions}\PY{p}{)}    
    \PY{n}{log\PYZus{}prob\PYZus{}actions} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{p}{)}
    \PY{n}{values} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{n}{values}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{n}{returns} \PY{o}{=} \PY{n}{calculate\PYZus{}returns}\PY{p}{(}\PY{n}{rewards}\PY{p}{,} \PY{n}{discount\PYZus{}factor}\PY{p}{)}
    \PY{n}{advantages} \PY{o}{=} \PY{n}{calculate\PYZus{}advantages}\PY{p}{(}\PY{n}{returns}\PY{p}{,} \PY{n}{values}\PY{p}{)}
    
    \PY{n}{policy\PYZus{}loss}\PY{p}{,} \PY{n}{value\PYZus{}loss} \PY{o}{=} \PY{n}{update\PYZus{}policy\PYZus{}ppo}\PY{p}{(}\PY{n}{agent}\PY{p}{,} \PY{n}{states}\PY{p}{,} \PY{n}{actions}\PY{p}{,} \PY{n}{log\PYZus{}prob\PYZus{}actions}\PY{p}{,} \PY{n}{advantages}\PY{p}{,} \PY{n}{returns}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{ppo\PYZus{}steps}\PY{p}{,} \PY{n}{ppo\PYZus{}clip}\PY{p}{)}

    \PY{k}{return} \PY{n}{policy\PYZus{}loss}\PY{p}{,} \PY{n}{value\PYZus{}loss}\PY{p}{,} \PY{n}{episode\PYZus{}reward}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{MAX\PYZus{}EPISODES} \PY{o}{=} \PY{l+m+mi}{5000}
\PY{n}{DISCOUNT\PYZus{}FACTOR} \PY{o}{=} \PY{l+m+mf}{0.99}
\PY{n}{N\PYZus{}TRIALS} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{REWARD\PYZus{}THRESHOLD} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{PPO\PYZus{}STEPS} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{PPO\PYZus{}CLIP} \PY{o}{=} \PY{l+m+mf}{0.2}

\PY{n}{policy\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{value\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{train\PYZus{}rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{test\PYZus{}rewards} \PY{o}{=} \PY{p}{[}\PY{p}{]}

\PY{k}{for} \PY{n}{episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{MAX\PYZus{}EPISODES}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    
    \PY{n}{policy\PYZus{}loss}\PY{p}{,} \PY{n}{value\PYZus{}loss}\PY{p}{,} \PY{n}{train\PYZus{}reward} \PY{o}{=} \PY{n}{train\PYZus{}ppo}\PY{p}{(}\PY{n}{train\PYZus{}env}\PY{p}{,} \PY{n}{agent}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{DISCOUNT\PYZus{}FACTOR}\PY{p}{,} \PY{n}{PPO\PYZus{}STEPS}\PY{p}{,} \PY{n}{PPO\PYZus{}CLIP}\PY{p}{)}
    
    \PY{n}{test\PYZus{}reward} \PY{o}{=} \PY{n}{evaluate}\PY{p}{(}\PY{n}{eval\PYZus{}env}\PY{p}{,} \PY{n}{agent}\PY{p}{)}
    
    \PY{n}{train\PYZus{}rewards}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{train\PYZus{}reward}\PY{p}{)}
    \PY{n}{test\PYZus{}rewards}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{test\PYZus{}reward}\PY{p}{)}
    \PY{n}{policy\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{policy\PYZus{}loss}\PY{p}{)}
    \PY{n}{value\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{value\PYZus{}loss}\PY{p}{)}

    
    \PY{n}{mean\PYZus{}train\PYZus{}rewards} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}rewards}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{N\PYZus{}TRIALS}\PY{p}{:}\PY{p}{]}\PY{p}{)}
    \PY{n}{mean\PYZus{}test\PYZus{}rewards} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}rewards}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{N\PYZus{}TRIALS}\PY{p}{:}\PY{p}{]}\PY{p}{)}
    
    \PY{k}{if} \PY{n}{episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{n}{plot}\PY{p}{(}\PY{n}{episode}\PY{p}{,} \PY{n}{train\PYZus{}rewards}\PY{p}{,} \PY{n}{policy\PYZus{}losses}\PY{p}{,} \PY{n}{value\PYZus{}losses}\PY{p}{)}
    
    \PY{k}{if} \PY{n}{mean\PYZus{}test\PYZus{}rewards} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{REWARD\PYZus{}THRESHOLD}\PY{p}{:}
        \PY{n}{plot}\PY{p}{(}\PY{n}{episode}\PY{p}{,} \PY{n}{train\PYZus{}rewards}\PY{p}{,} \PY{n}{policy\PYZus{}losses}\PY{p}{,} \PY{n}{value\PYZus{}losses}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Reached reward threshold in }\PY{l+s+si}{\PYZob{}}\PY{n}{episode}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ episodes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{k}{break}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_28_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Reached reward threshold in 2715 episodes
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
